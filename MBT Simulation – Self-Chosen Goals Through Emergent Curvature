MBT Simulation – Self-Chosen Goals Through Emergent Curvature

This simulation demonstrates a self-organizing MBT (Motion = Being Theory) field that evolves its own goals
through motion and resistance — without being explicitly told where to go or what to do.

Key concepts:
- The field receives random motion inputs (exploration).
- Memory accumulates where motion persists.
- Regions of low resistance and high memory become emergent “reward zones.”
- The system evolves to favor and reinforce paths toward these zones.
- There is no observer, no language, and no top-down guidance — just motion, resistance, and time.

This is the emergence of purpose through physics alone.


# MBT Curvature Field – Self-Chosen Goal Formation
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

# Parameters
grid_size = 120
timesteps = 400
dt = 0.1
pulse_amplitude = 1.0
decay_factor = 0.98
memory_decay = 0.999
learning_rate = 0.05
exploration_rate = 0.1  # chance to inject wave at random spot

# Initialize fields
field = np.zeros((grid_size, grid_size))
velocity = np.zeros_like(field)
memory = np.random.rand(grid_size, grid_size) * 0.005
reward_field = np.zeros_like(field)

# Visualization
fig, ax = plt.subplots(figsize=(7, 6))
im = ax.imshow(memory, cmap='inferno', vmin=0, vmax=0.05, animated=True)
ax.set_title("MBT Self-Organizing Field – Goal Emergence")

def laplacian(Z):
    return (
        -4 * Z
        + np.roll(Z, 1, axis=0) + np.roll(Z, -1, axis=0)
        + np.roll(Z, 1, axis=1) + np.roll(Z, -1, axis=1)
    )

def update(frame):
    global field, velocity, memory, reward_field

    # Exploration: random pulse injection
    if np.random.rand() < exploration_rate:
        y, x = np.random.randint(0, grid_size, size=2)
        field[y, x] = pulse_amplitude

    # Adaptive learning from curvature
    lap = laplacian(field)

    # Derive reward from low-resistance + high-memory zones
    reward_field = np.exp(-np.abs(field)) * memory
    reward_factor = 1 + reward_field * 0.5

    adaptive_factor = 1 + memory * learning_rate
    velocity += lap * dt * adaptive_factor * reward_factor
    velocity *= decay_factor
    field += velocity * dt

    memory = memory * memory_decay + np.abs(field) * (1 - memory_decay)
    im.set_array(memory)
    return [im]

ani = FuncAnimation(fig, update, frames=timesteps, interval=50, blit=True)
plt.show()

